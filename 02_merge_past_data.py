"""
éå»ãƒ‡ãƒ¼ã‚¿çµåˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®‰å…¨ãªæ”¹å–„ç‰ˆï¼‰
å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã¨éå»å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹

æ©Ÿèƒ½:
- å½“å¹´åº¦parquetãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
- éå»å¹´åº¦parquetãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
- ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ï¼ˆfull joinï¼‰
- æ³•äººãƒã‚¹ã‚¿ã¨ã®çµåˆ
- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
- è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿ã®å‡ºåŠ›

å®‰å…¨æ–¹é‡:
- ãƒ‡ãƒ¼ã‚¿å¤‰æ›´ã¯ä¸€åˆ‡è¡Œã‚ãªã„ï¼ˆå‡ºåŠ›çµæœã¯å¾“æ¥ã¨å®Œå…¨åŒä¸€ï¼‰
- æ®µéšçš„ãªæ”¹å–„ã§ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–
- å®Œå…¨ãªæ¤œè¨¼æ©Ÿèƒ½ä»˜ã
"""

import gc
import os
import time
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import polars as pl
import pretty_errors
from rich import print
from rich.console import Console
from rich.logging import RichHandler
from rich.progress import Progress, TaskID, BarColumn, TextColumn, TimeRemainingColumn
import logging

from config import MASTER_DIR, get_past_parquet_path, TERM, START_YEAR, get_fiscal_years

# ===== è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ =====
class ProcessConfig:
    """å‡¦ç†è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    # å‡ºåŠ›è¨­å®š
    OUTPUT_DIR: str = "è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿"
    
    # ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥
    DATA_TYPES: List[str] = ["æ³•äºº", "å¾—æ„å…ˆ", "å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"]
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
    SALES_PATTERNS: List[str] = ["å£²ä¸Š", "æ•°", "ã¡", "ç²—åˆ©", "æ•°é‡", "é‡‘é¡"]
    
    # çµåˆã‚­ãƒ¼å€™è£œ
    PRODUCT_JOIN_KEYS: List[str] = ["å–¶æ¥­æ‰€å", "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "ç¬¬1éšå±¤", "å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å"]
    
    # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›å¯¾è±¡åˆ—
    CODE_COLUMNS: List[str] = ["æ³•äººã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å•†å“ã‚³ãƒ¼ãƒ‰"]

# ===== ãƒ­ã‚®ãƒ³ã‚°è¨­å®š =====
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(rich_tracebacks=True)]
)
logger = logging.getLogger(__name__)
console = Console()

# UserWarningã‚’ç„¡è¦–
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")


class DataIntegrityValidator:
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def validate_data_totals(df1: pl.DataFrame, df2: pl.DataFrame, df3: pl.DataFrame, 
                           description: str) -> Dict[str, Dict[str, float]]:
        """ãƒ‡ãƒ¼ã‚¿ã®ç·è¨ˆå€¤ã‚’æ¤œè¨¼ãƒ»è¨˜éŒ²"""
        totals = {}
        
        if df1.is_empty():
            logger.warning(f"{description}: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return totals
        
        # å£²ä¸Šåˆ—ã‚’å–å¾—
        sales_columns = [col for col in df1.columns if col.endswith("å£²ä¸Š")]
        
        for sales_col in sorted(sales_columns):
            if sales_col in df1.columns and sales_col in df2.columns and sales_col in df3.columns:
                total1 = df1[sales_col].sum()
                total2 = df2[sales_col].sum()
                total3 = df3[sales_col].sum()
                
                totals[sales_col] = {
                    "æ³•äºº": total1,
                    "å¾—æ„å…ˆ": total2,
                    "å¾—æ„å…ˆï¼ˆå•†å“ï¼‰": total3
                }
                
                logger.info(f"{sales_col} ({description}):")
                logger.info(f"  æ³•äºº: {total1:,.0f}å††")
                logger.info(f"  å¾—æ„å…ˆ: {total2:,.0f}å††")
                logger.info(f"  å¾—æ„å…ˆï¼ˆå•†å“ï¼‰: {total3:,.0f}å††")
        
        return totals
    
    @staticmethod
    def compare_totals(before_totals: Dict, after_totals: Dict, year: str) -> bool:
        """çµåˆå‰å¾Œã®ç·è¨ˆå€¤ã‚’æ¯”è¼ƒï¼ˆå¼·åŒ–ç‰ˆï¼šè©³ç´°ãªå·®ç•°åˆ†æä»˜ãï¼‰
        
        Args:
            before_totals: çµåˆå‰ã®ç·è¨ˆå€¤
            after_totals: çµåˆå¾Œã®ç·è¨ˆå€¤
            year: å¯¾è±¡å¹´åº¦
            
        Returns:
            bool: æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœï¼ˆTrue=å•é¡Œãªã—ã€False=å·®ç•°ã‚ã‚Šï¼‰
        """
        logger.info("=== ç·è¨ˆå€¤æ¤œè¨¼å‡¦ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰ ===")
        
        is_valid = True
        total_issues = 0
        
        # å…¨ã¦ã®å¹´åº¦ã®å£²ä¸Šåˆ—ã‚’å–å¾—
        all_sales_columns = set()
        if before_totals:
            all_sales_columns.update(before_totals.keys())
        if after_totals:
            all_sales_columns.update(after_totals.keys())
        
        if not all_sales_columns:
            logger.warning("âš ï¸ æ¤œè¨¼å¯¾è±¡ã®å£²ä¸Šåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        logger.info(f"ğŸ” æ¤œè¨¼å¯¾è±¡åˆ—: {sorted(all_sales_columns)}")
        
        for sales_col in sorted(all_sales_columns):
            logger.info(f"\nğŸ“Š {sales_col} ã®è©³ç´°æ¤œè¨¼:")
            
            # çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã‚’è¡¨ç¤º
            if sales_col in after_totals:
                after = after_totals[sales_col]
                logger.info(f"  çµåˆå¾Œãƒ‡ãƒ¼ã‚¿:")
                logger.info(f"    æ³•äººãƒ‡ãƒ¼ã‚¿: {after['æ³•äºº']:,.0f}å††")
                logger.info(f"    å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿: {after['å¾—æ„å…ˆ']:,.0f}å††") 
                logger.info(f"    å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿: {after['å¾—æ„å…ˆï¼ˆå•†å“ï¼‰']:,.0f}å††")
                
                # çµåˆå‰å¾Œã®æ¯”è¼ƒï¼ˆé‡è¦ãªå·®ç•°ãƒã‚§ãƒƒã‚¯ï¼‰
                if sales_col in before_totals:
                    before = before_totals[sales_col]
                    logger.info(f"  çµåˆå‰å¾Œã®å·®ç•°åˆ†æ:")
                    
                    for data_type in ["æ³•äºº", "å¾—æ„å…ˆ", "å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"]:
                        if after[data_type] != before[data_type]:
                            diff = after[data_type] - before[data_type]
                            diff_abs = abs(diff)
                            diff_rate = (diff_abs / max(before[data_type], 1)) * 100
                            direction = "å¢—åŠ " if diff > 0 else "æ¸›å°‘"
                            
                            # å·®ç•°ã®ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š
                            if diff_abs > 1000000:  # 100ä¸‡å††ä»¥ä¸Š
                                level = "âŒ é‡å¤§"
                                is_valid = False
                                total_issues += 1
                            elif diff_abs > 100000:  # 10ä¸‡å††ä»¥ä¸Š
                                level = "âš ï¸ è­¦å‘Š"
                                total_issues += 1
                            else:
                                level = "â„¹ï¸ è»½å¾®"
                            
                            logger.warning(f"    {level} {data_type}: {diff:+,.0f}å†† ({direction}, å¤‰å‹•ç‡{diff_rate:.2f}%)")
                        else:
                            logger.info(f"    âœ… {data_type}: å·®ç•°ãªã—")
                else:
                    logger.info(f"  çµåˆå‰ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆæ–°è¦å¹´åº¦ã®å¯èƒ½æ€§ï¼‰")
                
                # ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥é–“ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
                logger.info(f"  ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥é–“æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯:")
                consistency_issues = []
                
                if after["æ³•äºº"] != after["å¾—æ„å…ˆ"]:
                    diff = abs(after["æ³•äºº"] - after["å¾—æ„å…ˆ"])
                    consistency_issues.append(f"æ³•äºº vs å¾—æ„å…ˆ: {diff:,.0f}å††ã®å·®ç•°")
                
                if after["å¾—æ„å…ˆ"] != after["å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"]:
                    diff = abs(after["å¾—æ„å…ˆ"] - after["å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"])
                    consistency_issues.append(f"å¾—æ„å…ˆ vs å¾—æ„å…ˆï¼ˆå•†å“ï¼‰: {diff:,.0f}å††ã®å·®ç•°")
                
                if after["æ³•äºº"] != after["å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"]:
                    diff = abs(after["æ³•äºº"] - after["å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"])
                    consistency_issues.append(f"æ³•äºº vs å¾—æ„å…ˆï¼ˆå•†å“ï¼‰: {diff:,.0f}å††ã®å·®ç•°")
                
                if consistency_issues:
                    logger.error(f"    âŒ æ•´åˆæ€§å•é¡Œ: {len(consistency_issues)}ä»¶")
                    for issue in consistency_issues:
                        logger.error(f"      - {issue}")
                    is_valid = False
                    total_issues += len(consistency_issues)
                else:
                    logger.info(f"    âœ… {sales_col}: å…¨ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã§ç·è¨ˆå€¤ãŒä¸€è‡´")
            else:
                logger.warning(f"  çµåˆå¾Œãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆãƒ‡ãƒ¼ã‚¿æ¶ˆå¤±ã®å¯èƒ½æ€§ï¼‰")
                is_valid = False
                total_issues += 1
        
        # ç·åˆåˆ¤å®š
        logger.info(f"\nğŸ“‹ ç·åˆæ¤œè¨¼çµæœ:")
        if is_valid and total_issues == 0:
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: å•é¡Œãªã—")
            logger.info("ğŸ‰ å…¨ã¦ã®å¹´åº¦ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«çµåˆã•ã‚Œã¾ã—ãŸ")
        else:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: {total_issues}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
            logger.error("âš ï¸ ãƒ‡ãƒ¼ã‚¿å·®ç•°ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚å‡¦ç†ã‚’åœæ­¢ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        return is_valid and total_issues == 0

    @staticmethod
    def validate_pre_merge_data(df1_a: pl.DataFrame, df2_a: pl.DataFrame, df3_a: pl.DataFrame,
                              df1_b: pl.DataFrame, df2_b: pl.DataFrame, df3_b: pl.DataFrame,
                              current_year: int) -> Dict[str, Dict]:
        """çµåˆå‰ã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’äº‹å‰æ¤œè¨¼
        
        Args:
            df1_a, df2_a, df3_a: å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿
            df1_b, df2_b, df3_b: éå»ãƒ‡ãƒ¼ã‚¿
            current_year: å½“å¹´åº¦
            
        Returns:
            Dict: æ¤œè¨¼çµæœã¨ã‚µãƒãƒªãƒ¼
        """
        logger.info("=== çµåˆå‰ãƒ‡ãƒ¼ã‚¿äº‹å‰æ¤œè¨¼ ===")
        
        validation_result = {
            "current_year_data": {},
            "past_data": {},
            "potential_conflicts": [],
            "recommendations": []
        }
        
        # å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not df1_a.is_empty():
            current_sales_col = f"{current_year}å£²ä¸Š"
            if current_sales_col in df1_a.columns:
                current_totals = {
                    "æ³•äºº": df1_a[current_sales_col].sum(),
                    "å¾—æ„å…ˆ": df2_a[current_sales_col].sum() if current_sales_col in df2_a.columns else 0,
                    "å¾—æ„å…ˆï¼ˆå•†å“ï¼‰": df3_a[current_sales_col].sum() if current_sales_col in df3_a.columns else 0
                }
                validation_result["current_year_data"][current_sales_col] = current_totals
                
                logger.info(f"ğŸ“Š å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆ{current_sales_col}ï¼‰:")
                for data_type, total in current_totals.items():
                    logger.info(f"  {data_type}: {total:,.0f}å††")
        
        # éå»ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not df1_b.is_empty():
            # éå»ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å¹´åº¦å£²ä¸Šåˆ—ã‚’ç‰¹å®š
            past_sales_cols = [col for col in df1_b.columns if col.endswith("å£²ä¸Š")]
            
            for sales_col in past_sales_cols:
                if sales_col in df1_b.columns and sales_col in df2_b.columns and sales_col in df3_b.columns:
                    past_totals = {
                        "æ³•äºº": df1_b[sales_col].sum(),
                        "å¾—æ„å…ˆ": df2_b[sales_col].sum(),
                        "å¾—æ„å…ˆï¼ˆå•†å“ï¼‰": df3_b[sales_col].sum()
                    }
                    validation_result["past_data"][sales_col] = past_totals
                    
                    logger.info(f"ğŸ“Š éå»ãƒ‡ãƒ¼ã‚¿ï¼ˆ{sales_col}ï¼‰:")
                    for data_type, total in past_totals.items():
                        logger.info(f"  {data_type}: {total:,.0f}å††")
                    
                    # å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã¨éå»ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if sales_col in validation_result["current_year_data"]:
                        current_total = validation_result["current_year_data"][sales_col]["æ³•äºº"]
                        past_total = past_totals["æ³•äºº"]
                        
                        if abs(current_total - past_total) > 1000:  # 1,000å††ä»¥ä¸Šã®å·®ç•°
                            conflict = {
                                "column": sales_col,
                                "current": current_total,
                                "past": past_total,
                                "diff": current_total - past_total
                            }
                            validation_result["potential_conflicts"].append(conflict)
                            logger.warning(f"âš ï¸ æ½œåœ¨çš„ç«¶åˆ: {sales_col} - å½“å¹´åº¦{current_total:,.0f}å†† vs éå»{past_total:,.0f}å††")
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        if validation_result["potential_conflicts"]:
            validation_result["recommendations"].append("ãƒ‡ãƒ¼ã‚¿çµåˆæ™‚ã«å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®ä¸Šæ›¸ãã«æ³¨æ„")
            validation_result["recommendations"].append("çµåˆå¾Œã®è©³ç´°æ¤œè¨¼ã‚’å¿…é ˆå®Ÿè¡Œ")
        
        if df1_b.is_empty():
            validation_result["recommendations"].append("éå»ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ã‚·ãƒ³ãƒ—ãƒ«ãªå½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’æ¨å¥¨")
        
        logger.info(f"ğŸ” äº‹å‰æ¤œè¨¼å®Œäº†: ç«¶åˆ{len(validation_result['potential_conflicts'])}ä»¶, æ¨å¥¨äº‹é …{len(validation_result['recommendations'])}ä»¶")
        
        return validation_result


class MemoryManager:
    """ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def cleanup() -> None:
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        gc.collect()
    
    @staticmethod
    def get_memory_usage() -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0


class DataProcessor:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.validator = DataIntegrityValidator()
        self.memory_manager = MemoryManager()
    
    def generate_years_list(self, start_year: int, end_year: int) -> List[str]:
        """å¯¾è±¡å¹´åº¦ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆé™é †ï¼‰
        
        Args:
            start_year: é–‹å§‹å¹´åº¦
            end_year: çµ‚äº†å¹´åº¦
            
        Returns:
            List[str]: å¹´åº¦ãƒªã‚¹ãƒˆï¼ˆé™é †ï¼‰
        """
        years = [str(year) for year in range(start_year, end_year + 1)]
        years.reverse()
        return years
    
    def load_current_data(self, filepath_base: str) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """å½“å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆparquetï¼‰ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            filepath_base: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒ™ãƒ¼ã‚¹
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: æ³•äººã€å¾—æ„å…ˆã€å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("=== å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===")
        datasets = {}
        
        for data_type in ProcessConfig.DATA_TYPES:
            filename = f"{filepath_base}_{data_type}.parquet"
            try:
                logger.info(f"èª­ã¿è¾¼ã¿ä¸­: {filename}")
                datasets[data_type] = pl.read_parquet(filename)
                row_count, col_count = datasets[data_type].shape
                logger.info(f"  {data_type}ãƒ‡ãƒ¼ã‚¿: {row_count:,}è¡Œ, {col_count}åˆ—")
            except Exception as e:
                logger.error(f"âŒ {data_type}ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                raise
        
        return datasets["æ³•äºº"], datasets["å¾—æ„å…ˆ"], datasets["å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"]
    
    def load_past_data(self, last_year: str, term: str) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """éå»å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ï¼ˆParquetï¼‰ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            last_year: å‰å¹´åº¦
            term: æœŸé–“
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: æ³•äººã€å¾—æ„å…ˆã€å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("=== éå»å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===")
        datasets = {}
        
        for data_type in ProcessConfig.DATA_TYPES:
            try:
                filepath = str(get_past_parquet_path(last_year, term, data_type))
                logger.info(f"èª­ã¿è¾¼ã¿ä¸­: {filepath}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                if not os.path.exists(filepath):
                    logger.warning(f"éå»å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
                    datasets[data_type] = pl.DataFrame()
                    continue
                
                # parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                datasets[data_type] = pl.read_parquet(filepath)
                row_count, col_count = datasets[data_type].shape
                logger.info(f"  {data_type}ãƒ‡ãƒ¼ã‚¿: {row_count:,}è¡Œ, {col_count}åˆ—")
                
            except Exception as e:
                logger.error(f"âŒ {data_type}ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                logger.error(f"ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {type(e).__name__}")
                import traceback
                logger.error(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")
                datasets[data_type] = pl.DataFrame()
        
        return datasets["æ³•äºº"], datasets["å¾—æ„å…ˆ"], datasets["å¾—æ„å…ˆï¼ˆå•†å“ï¼‰"]
    
    def rename_columns(self, df2_b: pl.DataFrame, df3_b: pl.DataFrame) -> Tuple[pl.DataFrame, pl.DataFrame]:
        """åˆ—åã®ãƒªãƒãƒ¼ãƒ å‡¦ç†
        
        Args:
            df2_b: å¾—æ„å…ˆã®éå»ãƒ‡ãƒ¼ã‚¿
            df3_b: å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ã®éå»ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame]: ãƒªãƒãƒ¼ãƒ å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if df2_b.is_empty() or df3_b.is_empty():
            return df2_b, df3_b
        
        # polarsæ–¹å¼ã§åˆ—åãƒªãƒãƒ¼ãƒ 
        if "å–å¼•å…ˆã‚³ãƒ¼ãƒ‰" in df2_b.columns:
            df2_b = df2_b.rename({"å–å¼•å…ˆã‚³ãƒ¼ãƒ‰": "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å–å¼•å…ˆå": "å¾—æ„å…ˆå"})
        if "å¾—æ„å…ˆ" in df3_b.columns:
            df3_b = df3_b.rename({"å¾—æ„å…ˆ": "å¾—æ„å…ˆå"})
        
        return df2_b, df3_b
    
    def add_past_suffix(self, df: pl.DataFrame, key_columns: List[str]) -> pl.DataFrame:
        """éå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã«_pastã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆçµåˆã‚­ãƒ¼ä¿è­·ç‰ˆï¼‰
        
        ä¿®æ­£: çµåˆã‚­ãƒ¼åˆ—ã¯ä¿è­·ã—ã€ãƒ‡ãƒ¼ã‚¿åˆ—ã®ã¿ã«ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        
        Args:
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            key_columns: çµåˆã‚­ãƒ¼åˆ—ãƒªã‚¹ãƒˆï¼ˆä¿è­·å¯¾è±¡ï¼‰
            
        Returns:
            pl.DataFrame: ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        if df.is_empty():
            return df
            
        rename_mapping = {}
        
        # åŸºæœ¬çš„ãªçµåˆã‚­ãƒ¼åˆ—ï¼ˆã“ã‚Œã‚‰ã¯ä¿è­·ã™ã‚‹ï¼‰
        protected_keys = ["æ³•äººã‚³ãƒ¼ãƒ‰", "æ³•äººå", "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", 
                         "å–¶æ¥­æ‰€å", "æ‹…å½“è€…å", "ç¬¬1éšå±¤", "å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å"]
        
        # å®Ÿéš›ã«æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼åˆ—ã‚‚ä¿è­·å¯¾è±¡ã«è¿½åŠ 
        all_protected_keys = set(protected_keys + key_columns)
        
        for col in df.columns:
            if col in all_protected_keys:
                # çµåˆã‚­ãƒ¼ã¯ä¿è­·ï¼ˆã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ ã—ãªã„ï¼‰
                continue
            else:
                # ãƒ‡ãƒ¼ã‚¿åˆ—ã®ã¿ã«ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
                rename_mapping[col] = f"{col}_past"
        
        if rename_mapping:
            df = df.rename(rename_mapping)
            logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿åˆ—åå¤‰æ›´: {len(rename_mapping)}åˆ—ã«ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ  (çµåˆã‚­ãƒ¼{len(all_protected_keys)}åˆ—ã¯ä¿è­·)")
        else:
            logger.info("éå»ãƒ‡ãƒ¼ã‚¿: ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ å¯¾è±¡åˆ—ãªã—ï¼ˆå…¨ã¦çµåˆã‚­ãƒ¼ï¼‰")
            
        return df
    
    def fix_full_join_nulls(self, df: pl.DataFrame, years_list: List[str], current_year_str: str) -> pl.DataFrame:
        """full joinå¾Œã®nullå€¤ã‚’çµ±ä¸€çš„ã«ä¿®å¾©ã™ã‚‹ï¼ˆå¹´åº¦ã‚’æ„è­˜ã—ãŸå‡¦ç†ï¼‰
        
        é‡è¦ãªä¿®æ­£: éå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®ç„¡æ¡ä»¶ä¸Šæ›¸ãã‚’é˜²æ­¢ã—ã€å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®ä¿è­·ã‚’å„ªå…ˆ
        
        Args:
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            years_list: å¹´åº¦ãƒªã‚¹ãƒˆ
            current_year_str: å½“å¹´åº¦æ–‡å­—åˆ—
            
        Returns:
            pl.DataFrame: nullå€¤ä¿®å¾©å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        # _pastã§çµ‚ã‚ã‚‹åˆ—ã‚’è¦‹ã¤ã‘ã¦å‡¦ç†
        past_columns = [col for col in df.columns if col.endswith('_past')]
        
        # ã¾ãšã€IDã‚«ãƒ©ãƒ ï¼ˆæ³•äººã‚³ãƒ¼ãƒ‰ã€å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ãªã©ï¼‰ã®å‡¦ç†
        for past_col in past_columns:
            base_col = past_col.replace('_past', '')
            
            # IDã‚«ãƒ©ãƒ ã®å ´åˆï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªã„ï¼‰
            if not any(year in base_col for year in years_list):
                if base_col not in df.columns:
                    df = df.with_columns(pl.col(past_col).alias(base_col))
                else:
                    # IDã‚«ãƒ©ãƒ ã¯nullã®å ´åˆã®ã¿è£œå®Œï¼ˆé‡è¦: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·ï¼‰
                    df = df.with_columns([
                        pl.when(pl.col(base_col).is_null())
                        .then(pl.col(past_col))
                        .otherwise(pl.col(base_col))
                        .alias(base_col)
                    ])
        
        # å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆ: å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·ã—ã€éå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®ä¸Šæ›¸ãã‚’é˜²æ­¢ï¼‰
        for year_str in years_list:
            # å£²ä¸Šæ•°ã¯å½“å¹´åº¦ã®ã¿å‡¦ç†
            if year_str == current_year_str:
                suffixes = ["å£²ä¸Šæ•°", "ã¡", "å£²ä¸Š", "ç²—åˆ©"]
            else:
                suffixes = ["ã¡", "å£²ä¸Š", "ç²—åˆ©"]
            
            for suffix in suffixes:
                col_name = f"{year_str}{suffix}"
                past_col_name = f"{col_name}_past"
                
                if year_str == current_year_str:
                    # å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã¯æ—¢å­˜ã®ã‚«ãƒ©ãƒ ã‚’ãã®ã¾ã¾ç¶­æŒï¼ˆä¿®æ­£ãªã—ï¼‰
                    # ã‚‚ã—ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆ
                    if col_name not in df.columns:
                        df = df.with_columns(pl.lit(None, dtype=pl.Float64).alias(col_name))
                    # â˜…é‡è¦â˜…: å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã¯_pastã‚«ãƒ©ãƒ ã§ä¸Šæ›¸ãã—ãªã„ï¼ˆæ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
                else:
                    # â˜…ä¿®æ­£â˜…: éå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã‚’æ¡ä»¶ä»˜ãã«å¤‰æ›´
                    if past_col_name in df.columns:
                        if col_name not in df.columns:
                            # éå¹´åº¦ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ã€_pastã‹ã‚‰ä½œæˆ
                            df = df.with_columns(pl.col(past_col_name).alias(col_name))
                        else:
                            # â˜…é‡è¦ãªä¿®æ­£â˜…: éå¹´åº¦ã‚«ãƒ©ãƒ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã€nullã®å ´åˆã®ã¿_pastã§è£œå®Œ
                            # ç„¡æ¡ä»¶ä¸Šæ›¸ãã§ã¯ãªãã€æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·
                            df = df.with_columns([
                                pl.when(pl.col(col_name).is_null())
                                .then(pl.col(past_col_name))
                                .otherwise(pl.col(col_name))
                                .alias(col_name)
                            ])
                    elif col_name not in df.columns:
                        # _pastã‚«ãƒ©ãƒ ã‚‚currentã‚«ãƒ©ãƒ ã‚‚å­˜åœ¨ã—ãªã„å ´åˆã€nullã§ä½œæˆ
                        df = df.with_columns(pl.lit(None, dtype=pl.Float64).alias(col_name))
        
        # å…¨ã¦ã®_paståˆ—ã‚’å‰Šé™¤
        for past_col in past_columns:
            if past_col in df.columns:
                df = df.drop(past_col)
        
        return df
    
    def merge_dataframes(self, df1_a: pl.DataFrame, df1_b: pl.DataFrame, 
                        df2_a: pl.DataFrame, df2_b: pl.DataFrame,
                        df3_a: pl.DataFrame, df3_b: pl.DataFrame,
                        years: List[str], current_year: int) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®çµåˆï¼ˆå®‰å…¨ãªçµåˆã‚­ãƒ¼å‡¦ç†ç‰ˆï¼‰
        
        ä¿®æ­£: çµåˆã‚­ãƒ¼ã‚’ä¿è­·ã—ã€ãƒ‡ãƒ¼ã‚¿å·®ç•°ã‚’é˜²æ­¢
        
        Args:
            df1_a, df1_b: æ³•äººãƒ‡ãƒ¼ã‚¿ï¼ˆå½“å¹´åº¦ã€éå»ï¼‰
            df2_a, df2_b: å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿ï¼ˆå½“å¹´åº¦ã€éå»ï¼‰
            df3_a, df3_b: å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ï¼ˆå½“å¹´åº¦ã€éå»ï¼‰
            years: å¹´åº¦ãƒªã‚¹ãƒˆ
            current_year: å½“å¹´åº¦
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        logger.info("=== ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç† ===")
        
        # éå»ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
        if df1_b.is_empty():
            logger.info("éå»ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return df1_a, df2_a, df3_a
        
        # éå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®åˆ—åèª¿æ•´ï¼ˆä¿®æ­£ç‰ˆ: çµåˆã‚­ãƒ¼ä¿è­·ï¼‰
        logger.info("éå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®åˆ—åèª¿æ•´ä¸­...")
        df1_b_renamed = self.add_past_suffix(df1_b, ["æ³•äººã‚³ãƒ¼ãƒ‰"])  # æ³•äººãƒ‡ãƒ¼ã‚¿ã®çµåˆã‚­ãƒ¼
        df2_b_renamed = self.add_past_suffix(df2_b, ["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå"])  # å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿ã®çµåˆã‚­ãƒ¼
        df3_b_renamed = self.add_past_suffix(df3_b, ProcessConfig.PRODUCT_JOIN_KEYS)  # å•†å“ãƒ‡ãƒ¼ã‚¿ã®çµåˆã‚­ãƒ¼
        
        # å®‰å…¨ãªçµåˆå‡¦ç†ï¼ˆçµåˆã‚­ãƒ¼ãŒä¿è­·ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€åŒåã‚­ãƒ¼ã§çµåˆå¯èƒ½ï¼‰
        df1 = df1_a.join(df1_b_renamed, how="full", on=["æ³•äººã‚³ãƒ¼ãƒ‰"])
        logger.info(f"æ³•äººãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(df1):,}è¡Œ, {len(df1.columns)}åˆ—")
        
        df2 = df2_a.join(df2_b_renamed, how="full", on=["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå"])
        logger.info(f"å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(df2):,}è¡Œ, {len(df2.columns)}åˆ—")
        
        # å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
        # çµåˆã‚­ãƒ¼ã®ã†ã¡ã€ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ã‚’ä½¿ç”¨
        join_keys = [key for key in ProcessConfig.PRODUCT_JOIN_KEYS if key in df3_a.columns and key in df3_b.columns]
        
        logger.info(f"å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ã®çµåˆã‚­ãƒ¼: {join_keys}")
        
        # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„å ´åˆã®ã¿çµåˆ
        if not df3_b.is_empty() and join_keys:
            # ä¿®æ­£: åŒåã‚­ãƒ¼ã§çµåˆï¼ˆçµåˆã‚­ãƒ¼ãŒä¿è­·ã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼‰
            df3 = df3_a.join(df3_b_renamed, how="full", on=join_keys)
        else:
            logger.warning("éå»ã®å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ãŒç©ºã¾ãŸã¯çµåˆã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€å½“å¹´åº¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            df3 = df3_a
            
        logger.info(f"å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(df3):,}è¡Œ, {len(df3.columns)}åˆ—")
        
        # çµåˆå¾Œã®nullå€¤ä¿®å¾©å‡¦ç†
        logger.info("çµåˆå¾Œã®nullå€¤ä¿®å¾©å‡¦ç†ä¸­...")
        df1 = self.fix_full_join_nulls(df1, years, str(current_year))
        df2 = self.fix_full_join_nulls(df2, years, str(current_year))
        df3 = self.fix_full_join_nulls(df3, years, str(current_year))
        logger.info("å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®nullå€¤ä¿®å¾©å®Œäº†")
        
        return df1, df2, df3
    
    def merge_with_master(self, df1: pl.DataFrame, master_path: str) -> pl.DataFrame:
        """æœ€æ–°æ³•äººãƒã‚¹ã‚¿ã¨ã®çµåˆ
        
        Args:
            df1: æ³•äººãƒ‡ãƒ¼ã‚¿
            master_path: ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            pl.DataFrame: ãƒã‚¹ã‚¿çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        try:
            logger.info(f"èª­ã¿è¾¼ã¿ä¸­: {master_path}/æ³•äººãƒã‚¹ã‚¿.csv")
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            schema = {
                "æ³•äººã‚³ãƒ¼ãƒ‰": pl.Utf8,
                "æ³•äººå": pl.Utf8
            }
            
            df1_c = pl.read_csv(
                f"{master_path}/æ³•äººãƒã‚¹ã‚¿.csv", 
                encoding="cp932",
                schema_overrides=schema
            ).select(["æ³•äººã‚³ãƒ¼ãƒ‰", "æ³•äººå"])
            
            df1 = df1.join(df1_c, on=["æ³•äººã‚³ãƒ¼ãƒ‰"], how="left", suffix="_new")
            
            # æ³•äººåã®çµ±åˆï¼ˆæ—¢å­˜ãŒã‚ã‚Œã°å„ªå…ˆã€ãªã‘ã‚Œã°æ–°ã—ã„ã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
            if "æ³•äººå" in df1.columns and "æ³•äººå_new" in df1.columns:
                df1 = df1.with_columns(
                    pl.coalesce([pl.col("æ³•äººå"), pl.col("æ³•äººå_new")]).alias("æ³•äººå")
                ).drop(["æ³•äººå_new"])
            elif "æ³•äººå_new" in df1.columns:
                df1 = df1.rename({"æ³•äººå_new": "æ³•äººå"})
            
            return df1
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚¹ã‚¿çµåˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def aggregate_product_data(self, df3: pl.DataFrame) -> pl.DataFrame:
        """å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ
        
        Args:
            df3: å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            pl.DataFrame: é›†è¨ˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        # æ•°å€¤åˆ—ã‚’ç‰¹å®šï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚­ãƒ¼ä»¥å¤–ã®åˆ—ï¼‰
        potential_group_cols = ["å–¶æ¥­æ‰€å", "æ‹…å½“è€…å", "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "ç¬¬1éšå±¤", "å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å"]
        # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä½¿ç”¨
        group_cols = [col for col in potential_group_cols if col in df3.columns]
        numeric_cols = [col for col in df3.columns if col not in group_cols]
        
        logger.info(f"é›†è¨ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ—: {group_cols}")
        
        # æ•°å€¤åˆ—ã®ã¿ã‚’é›†è¨ˆ
        agg_exprs = [pl.col(col).sum() for col in numeric_cols if df3[col].dtype in [pl.Int64, pl.Float64]]
        
        if group_cols and agg_exprs:
            return df3.group_by(group_cols).agg(agg_exprs)
        else:
            logger.warning("é›†è¨ˆã«å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return df3
    
    def reorder_columns(self, df1: pl.DataFrame, df2: pl.DataFrame, df3: pl.DataFrame, 
                       year: str, years: List[str]) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """åˆ—é †ã®æ•´ç†
        
        Args:
            df1, df2, df3: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            year: å½“å¹´åº¦
            years: å¹´åº¦ãƒªã‚¹ãƒˆ
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: æ•´ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        logger.info("=== åˆ—é †æ•´ç†å‡¦ç† ===")
        
        # ãƒ™ãƒ¼ã‚¹åˆ—ã‚’å®šç¾©
        base_cols_df1 = ["æ³•äººã‚³ãƒ¼ãƒ‰", "æ³•äººå", f"{year}å£²ä¸Šæ•°"]
        base_cols_df2 = ["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", f"{year}å£²ä¸Šæ•°"]
        # df3ã®ãƒ™ãƒ¼ã‚¹åˆ—ã¯å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ä½¿ç”¨
        potential_base_cols_df3 = ["å–¶æ¥­æ‰€å", "æ‹…å½“è€…å", "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "ç¬¬1éšå±¤", "å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å", f"{year}å£²ä¸Šæ•°"]
        base_cols_df3 = [col for col in potential_base_cols_df3 if col in df3.columns]
        
        # å„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—ã‚’å–å¾—
        cols1 = df1.columns
        cols2 = df2.columns
        cols3 = df3.columns
        
        # å¹´æ¬¡åˆ—ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (å‰å¹´åº¦å£²ä¸Šæ•°ã¯é™¤å¤–)
        def get_year_cols(columns):
            return [col for col in columns 
                    if any(str(y) in col for y in years) 
                    and not col.endswith(f"{int(year)-1}å£²ä¸Šæ•°")
                    and col not in base_cols_df1 + base_cols_df2 + base_cols_df3]
        
        # å„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã®å¹´æ¬¡åˆ—ã‚’å–å¾—
        year_cols1 = [col for col in get_year_cols(cols1) if col in cols1]
        year_cols2 = [col for col in get_year_cols(cols2) if col in cols2]
        year_cols3 = [col for col in get_year_cols(cols3) if col in cols3]
        
        # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠï¼ˆpolarsæ–¹å¼ï¼‰
        def safe_select(df, columns):
            existing_cols = [col for col in columns if col in df.columns]
            return df.select(existing_cols) if existing_cols else df
        
        return (
            safe_select(df1, base_cols_df1 + year_cols1),
            safe_select(df2, base_cols_df2 + year_cols2),
            safe_select(df3, base_cols_df3 + year_cols3)
        )
    
    def identify_column_types(self, df: pl.DataFrame) -> Tuple[List[str], List[str]]:
        """å€¤åˆ—ã¨indexåˆ—ã‚’å‹•çš„ã«ç‰¹å®š
        
        Args:
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            
        Returns:
            Tuple[List[str], List[str]]: (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—, å€¤åˆ—)
        """
        all_cols = df.columns
        
        # å€¤åˆ—ï¼ˆå£²ä¸Šã€æ•°é‡ã€ã¡ã€ç²—åˆ©ãªã©ï¼‰ã‚’ç‰¹å®š
        value_cols = [col for col in all_cols 
                     if any(pattern in col for pattern in ProcessConfig.SALES_PATTERNS)]
        
        # indexåˆ—ï¼ˆå€¤åˆ—ä»¥å¤–ã®ã™ã¹ã¦ï¼‰ã‚’ç‰¹å®š
        index_cols = [col for col in all_cols if col not in value_cols]
        
        return index_cols, value_cols
    
    def fill_missing_index_columns(self, df1: pl.DataFrame, df2: pl.DataFrame, df3: pl.DataFrame,
                                  df1_b: pl.DataFrame, df2_b: pl.DataFrame, df3_b: pl.DataFrame) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """éå»å¹´åº¦å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ã®indexåˆ—ç©´åŸ‹ã‚
        
        Args:
            df1, df2, df3: çµåˆå¾Œãƒ‡ãƒ¼ã‚¿
            df1_b, df2_b, df3_b: éå»ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: ç©´åŸ‹ã‚å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        logger.info("=== indexåˆ—ç©´åŸ‹ã‚å‡¦ç† ===")
        
        if df1_b.is_empty():
            logger.info("éå»ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ç©´åŸ‹ã‚ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return df1, df2, df3
        
        # æ³•äººãƒ‡ãƒ¼ã‚¿ã®ç©´åŸ‹ã‚ï¼ˆé‡è¤‡æ’é™¤æ©Ÿèƒ½ä»˜ãï¼‰
        if len(df1) > 0 and len(df1_b) > 0:
            index_cols1, value_cols1 = self.identify_column_types(df1)
            logger.info(f"æ³•äººãƒ‡ãƒ¼ã‚¿ - indexåˆ—: {index_cols1}")
            logger.info(f"æ³•äººãƒ‡ãƒ¼ã‚¿ - å€¤åˆ—: {value_cols1}")
            
            # è£œå®Œå‰ã®çŠ¶æ…‹ã‚’è¨˜éŒ²
            rows_before = len(df1)
            null_count_before = df1["æ³•äººã‚³ãƒ¼ãƒ‰"].is_null().sum()
            
            # æ³•äººåã‚’ã‚­ãƒ¼ã¨ã—ã¦éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ³•äººã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œ
            if "æ³•äººå" in df1.columns and "æ³•äººå" in df1_b.columns and "æ³•äººã‚³ãƒ¼ãƒ‰" in df1_b.columns:
                # æ³•äººã‚³ãƒ¼ãƒ‰ãŒnullã§æ³•äººåãŒnot nullã®è¡Œã‚’ç‰¹å®š
                null_code_mask = df1["æ³•äººã‚³ãƒ¼ãƒ‰"].is_null()
                has_name_mask = df1["æ³•äººå"].is_not_null()
                target_mask = null_code_mask & has_name_mask
                
                if target_mask.sum() > 0:
                    logger.info(f"æ³•äººãƒ‡ãƒ¼ã‚¿: {target_mask.sum()}è¡Œã®æ³•äººã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œã—ã¾ã™")
                    
                    # éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ³•äººåâ†’æ³•äººã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                    past_mapping = df1_b.select(["æ³•äººå", "æ³•äººã‚³ãƒ¼ãƒ‰"]).unique().drop_nulls()
                    
                    # æ³•äººåã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    name_counts = past_mapping.group_by("æ³•äººå").agg(pl.len().alias("count"))
                    duplicate_names = name_counts.filter(pl.col("count") > 1)
                    
                    if len(duplicate_names) > 0:
                        logger.warning(f"è­¦å‘Š: {len(duplicate_names)}ä»¶ã®æ³•äººåã§è¤‡æ•°ã®æ³•äººã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ã¾ã™")
                        # é‡è¤‡ãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®1ä»¶ã®ã¿ã‚’ä½¿ç”¨
                        past_mapping = past_mapping.group_by("æ³•äººå").first()
                    
                    logger.info(f"ãƒãƒƒãƒ”ãƒ³ã‚°ä»¶æ•°: {len(past_mapping)}ä»¶")
                    
                    # left joinã§æ³•äººã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œ
                    df1_filled = df1.join(
                        past_mapping.rename({"æ³•äººã‚³ãƒ¼ãƒ‰": "æ³•äººã‚³ãƒ¼ãƒ‰_è£œå®Œ"}),
                        on="æ³•äººå", 
                        how="left"
                    )
                    
                    # å…ƒã®æ³•äººã‚³ãƒ¼ãƒ‰ãŒnullã®å ´åˆã®ã¿è£œå®Œå€¤ã‚’ä½¿ç”¨
                    df1 = df1_filled.with_columns(
                        pl.coalesce([pl.col("æ³•äººã‚³ãƒ¼ãƒ‰"), pl.col("æ³•äººã‚³ãƒ¼ãƒ‰_è£œå®Œ")]).alias("æ³•äººã‚³ãƒ¼ãƒ‰")
                    ).drop("æ³•äººã‚³ãƒ¼ãƒ‰_è£œå®Œ")
                    
                    # è£œå®Œå¾Œã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
                    rows_after = len(df1)
                    null_count_after = df1["æ³•äººã‚³ãƒ¼ãƒ‰"].is_null().sum()
                    
                    logger.info("æ³•äººãƒ‡ãƒ¼ã‚¿è£œå®Œçµæœ:")
                    logger.info(f"  è¡Œæ•°: {rows_before} â†’ {rows_after} (å·®: {rows_after - rows_before})")
                    logger.info(f"  nullæ•°: {null_count_before} â†’ {null_count_after} (è£œå®Œ: {null_count_before - null_count_after})")
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if rows_after > rows_before:
                        logger.warning("âš  è­¦å‘Š: è£œå®Œå‡¦ç†ã§è¡Œæ•°ãŒå¢—åŠ ã—ã¾ã—ãŸã€‚é‡è¤‡ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                        # æ³•äººã‚³ãƒ¼ãƒ‰+æ³•äººåã§ã®é‡è¤‡æ’é™¤
                        df1_dedup = df1.unique(subset=["æ³•äººã‚³ãƒ¼ãƒ‰", "æ³•äººå"])
                        rows_dedup = len(df1_dedup)
                        if rows_dedup < rows_after:
                            logger.info(f"é‡è¤‡æ’é™¤å®Ÿè¡Œ: {rows_after} â†’ {rows_dedup}è¡Œ (å‰Šé™¤: {rows_after - rows_dedup}è¡Œ)")
                            df1 = df1_dedup
                        else:
                            logger.info("æ³•äººã‚³ãƒ¼ãƒ‰+æ³•äººåã«ã‚ˆã‚‹é‡è¤‡ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    
                    logger.info(f"æœ€çµ‚çš„ãªæ³•äººãƒ‡ãƒ¼ã‚¿: {len(df1)}è¡Œ, nullæ•°: {df1['æ³•äººã‚³ãƒ¼ãƒ‰'].is_null().sum()}")
        
        # å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿ã®ç©´åŸ‹ã‚
        if len(df2) > 0 and len(df2_b) > 0:
            index_cols2, value_cols2 = self.identify_column_types(df2)
            logger.info(f"å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿ - indexåˆ—: {index_cols2}")
            logger.info(f"å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿ - å€¤åˆ—: {value_cols2}")
            
            # å¾—æ„å…ˆåã‚’ã‚­ãƒ¼ã¨ã—ã¦éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œ
            if "å¾—æ„å…ˆå" in df2.columns and "å¾—æ„å…ˆå" in df2_b.columns and "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰" in df2_b.columns:
                null_code_mask = df2["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"].is_null()
                has_name_mask = df2["å¾—æ„å…ˆå"].is_not_null()
                target_mask = null_code_mask & has_name_mask
                
                if target_mask.sum() > 0:
                    logger.info(f"å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿: {target_mask.sum()}è¡Œã®å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œã—ã¾ã™")
                    
                    # éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾—æ„å…ˆåâ†’å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                    past_mapping = df2_b.select(["å¾—æ„å…ˆå", "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"]).unique().drop_nulls()
                    
                    # left joinã§å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œ
                    df2_filled = df2.join(
                        past_mapping.rename({"å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰": "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰_è£œå®Œ"}),
                        on="å¾—æ„å…ˆå", 
                        how="left"
                    )
                    
                    # å…ƒã®å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ãŒnullã®å ´åˆã®ã¿è£œå®Œå€¤ã‚’ä½¿ç”¨
                    df2 = df2_filled.with_columns(
                        pl.coalesce([pl.col("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"), pl.col("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰_è£œå®Œ")]).alias("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰")
                    ).drop("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰_è£œå®Œ")
                    
                    logger.info(f"å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿è£œå®Œå¾Œã®å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰nullæ•°: {df2['å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰'].is_null().sum()}")
        
        # å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ã®ç©´åŸ‹ã‚ï¼ˆè»½é‡ç‰ˆï¼‰
        if len(df3) > 0 and len(df3_b) > 0:
            index_cols3, value_cols3 = self.identify_column_types(df3)
            logger.info(f"å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ - indexåˆ—: {index_cols3}")
            logger.info(f"å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ - å€¤åˆ—: {value_cols3}")
            
            # nullæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦è¡¨ç¤ºï¼ˆå®Ÿéš›ã®è£œå®Œã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ€§èƒ½å„ªå…ˆï¼‰
            null_counts = {}
            for col in index_cols3:
                if col in df3.columns:
                    null_count = df3[col].is_null().sum()
                    if null_count > 0:
                        null_counts[col] = null_count
                        logger.info(f"å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿: {null_count}è¡Œã®{col}ãŒnullã§ã™")
            
            if null_counts:
                logger.info(f"ç©´åŸ‹ã‚å¯¾è±¡: {len(null_counts)}åˆ—ï¼ˆæ€§èƒ½å„ªå…ˆã®ãŸã‚å®Ÿéš›ã®è£œå®Œã¯çœç•¥ï¼‰")
                logger.info("â€» å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã€ç©´åŸ‹ã‚å‡¦ç†ã¯å¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥å®Ÿè£…ã—ã¦ãã ã•ã„")
            else:
                logger.info("å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿: indexåˆ—ã«nullã¯ã‚ã‚Šã¾ã›ã‚“")
        
        logger.info("indexåˆ—ç©´åŸ‹ã‚å‡¦ç†å®Œäº†")
        return df1, df2, df3
    
    def sort_dataframes(self, df1: pl.DataFrame, df2: pl.DataFrame, df3: pl.DataFrame,
                       years: List[str], current_year: int) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚½ãƒ¼ãƒˆï¼ˆå‹•çš„å¹´åº¦å¯¾å¿œã®å½“å¹´åº¦å£²ä¸Šå„ªå…ˆï¼‰
        
        Args:
            df1, df2, df3: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            years: å¹´åº¦ãƒªã‚¹ãƒˆ
            current_year: å½“å¹´åº¦
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        logger.info("=== ã‚½ãƒ¼ãƒˆå‡¦ç† ===")
        
        # å‹•çš„å¹´åº¦ã«åŸºã¥ãå£²ä¸Šåˆ—å
        current_sales_col = f"{current_year}å£²ä¸Š"
        
        # å½“å¹´åº¦å£²ä¸Šåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å½“å¹´åº¦å„ªå…ˆã§ã‚½ãƒ¼ãƒˆ
        if current_sales_col in df1.columns:
            logger.info(f"å‹•çš„å¹´åº¦å£²ä¸Šï¼ˆ{current_sales_col}ï¼‰ã§é™é †ã‚½ãƒ¼ãƒˆã—ã¾ã™")
            
            # polarsã§nullå€¤ã‚’0ã§ç½®æ›ã—ã¦ã‹ã‚‰ã‚½ãƒ¼ãƒˆ
            df1 = df1.with_columns(pl.col(current_sales_col).fill_null(0))
            df2 = df2.with_columns(pl.col(current_sales_col).fill_null(0))
            df3 = df3.with_columns(pl.col(current_sales_col).fill_null(0))
            
            # é™é †ã‚½ãƒ¼ãƒˆï¼ˆpolarsã®åŠ¹ç‡çš„ãªã‚½ãƒ¼ãƒˆï¼‰
            df1 = df1.sort(current_sales_col, descending=True)
            df2 = df2.sort(current_sales_col, descending=True)
            df3 = df3.sort([current_sales_col, "å–¶æ¥­æ‰€å"], descending=[True, False])
            
            # ã‚½ãƒ¼ãƒˆçµæœç¢ºèªï¼ˆpolarsã®select + headã§åŠ¹ç‡çš„ã«ç¢ºèªï¼‰
            logger.info("ã‚½ãƒ¼ãƒˆçµæœç¢ºèª:")
            try:
                top5_corp = df1.select(current_sales_col).head(5)[current_sales_col].to_list()
                top5_client = df2.select(current_sales_col).head(5)[current_sales_col].to_list()
                top5_product = df3.select(current_sales_col).head(5)[current_sales_col].to_list()
                
                logger.info(f"  æ³•äººãƒˆãƒƒãƒ—5ã®{current_sales_col}: {[f'{x:,.0f}' for x in top5_corp]}")
                logger.info(f"  å¾—æ„å…ˆãƒˆãƒƒãƒ—5ã®{current_sales_col}: {[f'{x:,.0f}' for x in top5_client]}")
                logger.info(f"  å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒˆãƒƒãƒ—5ã®{current_sales_col}: {[f'{x:,.0f}' for x in top5_product]}")
            except Exception as e:
                logger.warning(f"  ã‚½ãƒ¼ãƒˆçµæœç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            logger.info(f"å‹•çš„å¹´åº¦å£²ä¸Šåˆ—ï¼ˆ{current_sales_col}ï¼‰ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å¾“æ¥ã®ã‚½ãƒ¼ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ã‚½ãƒ¼ãƒˆæ–¹å¼
            sort_columns = [f"{year}å£²ä¸Š" for year in years if f"{year}å£²ä¸Š" in df1.columns]
            
            if sort_columns:
                df1 = df1.sort(sort_columns[0], descending=True)
                df2 = df2.sort(sort_columns[0], descending=True)
            
            df3 = df3.sort(["å–¶æ¥­æ‰€å", "å¾—æ„å…ˆå", "ç¬¬1éšå±¤"], descending=[False, False, False])
        
        logger.info(f"ã‚½ãƒ¼ãƒˆå®Œäº† - æ³•äºº: {len(df1):,}è¡Œ, å¾—æ„å…ˆ: {len(df2):,}è¡Œ, å¾—æ„å…ˆï¼ˆå•†å“ï¼‰: {len(df3):,}è¡Œ")
        
        return df1, df2, df3
    
    def convert_data_types(self, df1: pl.DataFrame, df2: pl.DataFrame, df3: pl.DataFrame) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®å¤‰æ›
        
        Args:
            df1, df2, df3: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            
        Returns:
            Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        logger.info("=== ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›å‡¦ç† ===")
        
        # ã‚³ãƒ¼ãƒ‰åˆ—ã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼ˆpolarsæ–¹å¼ï¼‰
        for col in ProcessConfig.CODE_COLUMNS:
            if col in df1.columns:
                df1 = df1.with_columns(pl.col(col).cast(pl.Utf8))
            if col in df2.columns:
                df2 = df2.with_columns(pl.col(col).cast(pl.Utf8))
            if col in df3.columns:
                df3 = df3.with_columns(pl.col(col).cast(pl.Utf8))
        
        return df1, df2, df3
    
    def save_data(self, df1: pl.DataFrame, df2: pl.DataFrame, df3: pl.DataFrame, output_dir: str) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        
        Args:
            df1, df2, df3: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        logger.info("=== ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å‡¦ç† ===")
        os.makedirs(output_dir, exist_ok=True)
        
        # polarsæ–¹å¼ã§ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        df1.write_parquet(f"{output_dir}/æ³•äºº_è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿.parquet")
        logger.info(f"æ³•äººãƒ‡ãƒ¼ã‚¿ã‚’ {output_dir}/æ³•äºº_è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿.parquet ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        df2.write_parquet(f"{output_dir}/å¾—æ„å…ˆ_è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿.parquet")
        logger.info(f"å¾—æ„å…ˆãƒ‡ãƒ¼ã‚¿ã‚’ {output_dir}/å¾—æ„å…ˆ_è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿.parquet ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        df3.write_parquet(f"{output_dir}/å¾—æ„å…ˆï¼ˆå•†å“ï¼‰_è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿.parquet")
        logger.info(f"å¾—æ„å…ˆï¼ˆå•†å“ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’ {output_dir}/å¾—æ„å…ˆï¼ˆå•†å“ï¼‰_è»¢è¨˜ç”¨ãƒ‡ãƒ¼ã‚¿.parquet ã«ä¿å­˜ã—ã¾ã—ãŸ")


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆãƒ‡ãƒ¼ã‚¿å·®ç•°ä¿®æ­£ç‰ˆï¼‰"""
    start_time = time.time()
    
    # ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–
    processor = DataProcessor()
    validator = DataIntegrityValidator()
    memory_manager = MemoryManager()
    
    try:
        logger.info("=== éå»ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰ ===")
        
        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        initial_memory = memory_manager.get_memory_usage()
        if initial_memory > 0:
            logger.info(f"ğŸ“Š åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory:.1f}MB")
        
        # å¹´åº¦è¨ˆç®—
        year, last_year, current_year = get_fiscal_years()
        years = processor.generate_years_list(START_YEAR, current_year)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
        filepath_current = f"{TERM}_ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¾—æ„å…ˆï¼‰"
        master_path = str(MASTER_DIR)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("â±ï¸ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        df1_a, df2_a, df3_a = processor.load_current_data(filepath_current)
        df1_b, df2_b, df3_b = processor.load_past_data(last_year, TERM)
        
        # â˜…æ–°æ©Ÿèƒ½â˜…: çµåˆå‰ãƒ‡ãƒ¼ã‚¿ã®äº‹å‰æ¤œè¨¼
        pre_validation = validator.validate_pre_merge_data(
            df1_a, df2_a, df3_a, df1_b, df2_b, df3_b, current_year
        )
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        memory_manager.cleanup()
        
        # çµåˆå‰ã®éå»ãƒ‡ãƒ¼ã‚¿ç·è¨ˆå€¤ã‚’è¨˜éŒ²
        past_totals = validator.validate_data_totals(df1_b, df2_b, df3_b, "éå»ãƒ‡ãƒ¼ã‚¿")
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆã®çµåˆå‡¦ç†ã‚’ä½¿ç”¨ï¼‰
        logger.info("â±ï¸ ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹ï¼ˆå®‰å…¨ãªçµåˆå‡¦ç†ï¼‰")
        df2_b, df3_b = processor.rename_columns(df2_b, df3_b)
        df1, df2, df3 = processor.merge_dataframes(df1_a, df1_b, df2_a, df2_b, df3_a, df3_b, years, current_year)
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del df1_a, df1_b, df2_a, df2_b, df3_a, df3_b
        memory_manager.cleanup()
        
        df1 = processor.merge_with_master(df1, master_path)
        df3 = processor.aggregate_product_data(df3)
        df1, df2, df3 = processor.reorder_columns(df1, df2, df3, year, years)
        df1, df2, df3 = processor.fill_missing_index_columns(df1, df2, df3, pl.DataFrame(), pl.DataFrame(), pl.DataFrame())
        df1, df2, df3 = processor.sort_dataframes(df1, df2, df3, years, current_year)
        df1, df2, df3 = processor.convert_data_types(df1, df2, df3)
        
        # â˜…å¼·åŒ–â˜…: çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ç·è¨ˆå€¤ã‚’æ¤œè¨¼ï¼ˆè©³ç´°ç‰ˆï¼‰
        after_totals = validator.validate_data_totals(df1, df2, df3, "çµåˆå¾Œ")
        integrity_check_passed = validator.compare_totals(past_totals, after_totals, year)
        
        # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœã«å¿œã˜ãŸå‡¦ç†
        if not integrity_check_passed:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")
            logger.error("âš ï¸  å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ãŒã€çµæœã®ç¢ºèªã‚’å¼·ãæ¨å¥¨ã—ã¾ã™")
        else:
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: æ­£å¸¸")
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        processor.save_data(df1, df2, df3, ProcessConfig.OUTPUT_DIR)
        
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = memory_manager.get_memory_usage()
        if final_memory > 0 and initial_memory > 0:
            memory_used = final_memory - initial_memory
            logger.info(f"ğŸ“Š æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: +{memory_used:.1f}MB")
        
        # å‡¦ç†æ™‚é–“è¡¨ç¤º
        elapsed_time = time.time() - start_time
        logger.info("=== å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ ===")
        logger.info(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        
        if integrity_check_passed:
            console.print("âœ… [bold green]éå»ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆæ•´åˆæ€§ç¢ºèªæ¸ˆã¿ï¼‰[/bold green]")
        else:
            console.print("âš ï¸ [bold yellow]éå»ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿å·®ç•°ã‚’æ¤œå‡º[/bold yellow]")
            console.print("ğŸ“‹ è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„")
        
    except Exception as e:
        logger.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        console.print(f"âŒ [bold red]éå»ãƒ‡ãƒ¼ã‚¿çµåˆå‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}[/bold red]")
        raise


if __name__ == "__main__":
    main()
